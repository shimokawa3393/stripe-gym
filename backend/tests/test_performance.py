"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼šãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ãƒ†ã‚¹ãƒˆ
"""
import pytest
import time
import concurrent.futures
import statistics


@pytest.mark.slow
def test_api_response_time(client):
    """APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®ãƒ†ã‚¹ãƒˆ"""
    
    endpoints_to_test = [
        ('/health', 'GET'),
    ]
    
    for endpoint, method in endpoints_to_test:
        execution_times = []
        
        # 10å›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã—ã¦å¹³å‡æ™‚é–“ã‚’è¨ˆç®—
        for _ in range(10):
            start_time = time.time()
            
            if method == 'GET':
                response = client.get(endpoint)
            elif method == 'POST':
                response = client.post(endpoint, json={})
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            execution_times.append(execution_time)
            
            # å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæˆåŠŸã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            assert response.status_code in [200, 404, 405], f"ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ {endpoint} ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—: {response.status_code}"
        
        # çµ±è¨ˆã‚’è¨ˆç®—
        avg_time = statistics.mean(execution_times)
        max_time = max(execution_times)
        min_time = min(execution_times)
        
        print(f"ğŸ“Š {endpoint} ({method}): å¹³å‡={avg_time:.3f}s, æœ€å¤§={max_time:.3f}s, æœ€å°={min_time:.3f}s")
        
        # å¹³å‡å¿œç­”æ™‚é–“ãŒ1ç§’ä»¥å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert avg_time < 1.0, f"ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ {endpoint} ã®å¹³å‡å¿œç­”æ™‚é–“ãŒé…ã™ãã¾ã™: {avg_time:.3f}s"
        
        # æœ€å¤§å¿œç­”æ™‚é–“ãŒ5ç§’ä»¥å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert max_time < 5.0, f"ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ {endpoint} ã®æœ€å¤§å¿œç­”æ™‚é–“ãŒé…ã™ãã¾ã™: {max_time:.3f}s"


@pytest.mark.slow
def test_concurrent_requests(client):
    """åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ã®ãƒ†ã‚¹ãƒˆ"""
    
    def make_request():
        """å˜ä¸€ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡"""
        start_time = time.time()
        response = client.get('/health')
        end_time = time.time()
        return {
            'status_code': response.status_code,
            'execution_time': end_time - start_time
        }
    
    # åŒæ™‚ã«10å€‹ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
    concurrent_requests = 10
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_requests) as executor:
        futures = [executor.submit(make_request) for _ in range(concurrent_requests)]
        results = [future.result() for future in concurrent.futures.as_completed(futures)]
    
    end_time = time.time()
    total_execution_time = end_time - start_time
    
    # å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæˆåŠŸã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    success_count = sum(1 for result in results if result['status_code'] == 200)
    assert success_count >= concurrent_requests * 0.8, f"åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®æˆåŠŸç‡ãŒä½ã™ãã¾ã™: {success_count}/{concurrent_requests}"
    
    # å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å¹³å‡å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
    avg_execution_time = statistics.mean([result['execution_time'] for result in results])
    max_execution_time = max(result['execution_time'] for result in results)
    
    print(f"ğŸ“Š åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ†ã‚¹ãƒˆ: æˆåŠŸç‡={success_count}/{concurrent_requests}, å¹³å‡æ™‚é–“={avg_execution_time:.3f}s, æœ€å¤§æ™‚é–“={max_execution_time:.3f}s")
    
    # å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒ500msä»¥å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert avg_execution_time < 0.5, f"åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒé…ã™ãã¾ã™: {avg_execution_time:.3f}s"


@pytest.mark.slow
def test_database_query_performance(db_session):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    execution_times = []
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®å¿œç­”æ™‚é–“ã‚’æ¸¬å®š
    for _ in range(20):
        start_time = time.time()
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        users_count = db_session.query(db_session.query().table_name == 'users').count()
        
        # è¤‡é›‘ãªJOINã‚¯ã‚¨ãƒªå®Ÿè¡Œ
        results = db_session.query().filter().all()
        
        end_time = time.time()
        execution_time = end_time - start_time
        execution_times.append(execution_time)
    
    avg_time = statistics.mean(execution_times)
    max_time = max(execution_times)
    
    print(f"ğŸ“Š DBã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: å¹³å‡={avg_time:.3f}s, æœ€å¤§={max_time:.3f}s")
    
    # å¹³å‡ã‚¯ã‚¨ãƒªæ™‚é–“ãŒ100msä»¥å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert avg_time < 0.1, f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒé…ã™ãã¾ã™: {avg_time:.3f}s"


@pytest.mark.slow 
@pytest.mark.integration
def test_end_to_end_performance(client, sample_user):
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    execution_times = []
    
    for _ in range(5):  # 5å›ã®ãƒ•ãƒ«ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ
        start_time = time.time()
        
        # 1. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
        health_response = client.get('/health')
        assert health_response.status_code == 200
        
        # 2. APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        register_response = client.get('/api/register')
        assert register_response.status_code == 405  # Method Not Allowedï¼ˆæ­£å¸¸ï¼‰
        
        # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ç¢ºèªï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
        user_check_time = time.time()
        if hasattr(sample_user, 'id'):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®ç¢ºèª
            user_id = sample_user.id
            # å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ç¢ºèªã¯å®Ÿè£…æ¬¡ç¬¬
        user_check_response_time = time.time() - user_check_time
        
        end_time = time.time()
        total_execution_time = end_time - start_time
        execution_times.append(total_execution_time)
        
        print(f"ğŸ“Š E2Eãƒ•ãƒ­ãƒ¼ {_}å›ç›®: {total_execution_time:.3f}s (ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª: {user_check_response_time:.3f}s)")
    
    avg_time = statistics.mean(execution_times)
    max_time = max(execution_times)
    
    print(f"ğŸ“Š å…¨ä½“E2Eãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: å¹³å‡={avg_time:.3f}s, æœ€å¤§={max_time:.3f}s")
    
    # å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒ2ç§’ä»¥å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert avg_time < 2.0, f"ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ•ãƒ­ãƒ¼ã®å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒé…ã™ãã¾ã™: {avg_time:.3f}s"


@pytest.mark.slow
def test_memory_usage():
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ†ã‚¹ãƒˆ"""
    import psutil
    import gc
    
    # ã‚¬ãƒ™ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    gc.collect()
    
    # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã‚’å–å¾—
    process = psutil.Process()
    memory_info = process.memory_info()
    memory_mb = memory_info.rss / 1024 / 1024  # MBå˜ä½ã«å¤‰æ›
    
    print(f"ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_mb:.2f}MB")
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ100MBä»¥å†…ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ï¼‰
    assert memory_mb < 100, f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§ãã™ãã¾ã™: {memory_mb:.2f}MB"


@pytest.mark.slow
def test_scalability_initial_indicators(client):
    """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®åˆæœŸæŒ‡æ¨™ãƒ†ã‚¹ãƒˆ"""
    
    # è¤‡æ•°ã®ãƒªã‚¨ãƒ³ã‚¯ã‚¨ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œ
    request_counts = [1, 5, 10, 20]
    results = {}
    
    for count in request_counts:
        execution_times = []
        success_count = 0
        
        start_time = time.time()
        
        for _ in range(count):
            req_start = time.time()
            response = client.get('/health')
            req_end = time.time()
            
            execution_times.append(req_end - req_start)
            
            if response.status_code == 200:
                success_count += 1
        
        total_time = time.time() - start_time
        avg_time = statistics.mean(execution_times)
        
        results[count] = {
            'total_time': total_time,
            'avg_time': avg_time,
            'success_rate': success_count / count,
            'requests_per_second': count / total_time
        }
        
        print(f"ğŸ“Š ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° {count}: RPS={results[count]['requests_per_second']:.2f}, æˆåŠŸç‡={results[count]['success_rate']:.2%}")
    
    # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãŒå¢—ãˆã¦ã‚‚æˆåŠŸç‡ãŒ80%ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert results[20]['success_rate'] >= 0.8, f"20åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®æˆåŠŸç‡ãŒä½ã™ãã¾ã™: {results[20]['success_rate']:.2%}"
    
    # RPSãŒã‚ã‚‹ç¨‹åº¦ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    assert results[20]['requests_per_second'] >= 5, f"20åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚ã®RPSãŒä½ã™ãã¾ã™: {results[20]['requests_per_second']:.2f}"
